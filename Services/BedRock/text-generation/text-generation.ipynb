{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb8bc51c-8362-47fb-bca2-490d4cab908e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:41:03.135729Z",
     "iopub.status.busy": "2025-06-21T13:41:03.135362Z",
     "iopub.status.idle": "2025-06-21T13:41:03.139613Z",
     "shell.execute_reply": "2025-06-21T13:41:03.138778Z",
     "shell.execute_reply.started": "2025-06-21T13:41:03.135707Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import botocore\n",
    "from IPython.display import display, Markdown\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fda6e414-aaeb-495c-b168-b0e77f673fcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:41:04.175010Z",
     "iopub.status.busy": "2025-06-21T13:41:04.174674Z",
     "iopub.status.idle": "2025-06-21T13:41:04.192891Z",
     "shell.execute_reply": "2025-06-21T13:41:04.191973Z",
     "shell.execute_reply.started": "2025-06-21T13:41:04.174988Z"
    }
   },
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "bedrock = boto3.client(service_name='bedrock-runtime', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d79528da-bf89-4486-a1c4-0163b893eb3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:43:29.499319Z",
     "iopub.status.busy": "2025-06-21T13:43:29.499044Z",
     "iopub.status.idle": "2025-06-21T13:43:29.504596Z",
     "shell.execute_reply": "2025-06-21T13:43:29.503909Z",
     "shell.execute_reply.started": "2025-06-21T13:43:29.499296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ap-south-1'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# session # --> Session(region_name='ap-south-1')\n",
    "# region  # --> 'ap-south-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b1d0717-c784-47aa-9a99-ad5b1bb7334a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:43:50.459023Z",
     "iopub.status.busy": "2025-06-21T13:43:50.458104Z",
     "iopub.status.idle": "2025-06-21T13:43:50.462429Z",
     "shell.execute_reply": "2025-06-21T13:43:50.461595Z",
     "shell.execute_reply.started": "2025-06-21T13:43:50.458994Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define model IDs that will be used in this module\n",
    "MODELS = {\n",
    "    \"Titan Text G1 - Lite\": \"amazon.titan-text-lite-v1\",\n",
    "    \"Titan Text G1 - Express\": \"amazon.titan-text-express-v1\",\n",
    "\n",
    "    # -- not having access for these models\n",
    "    \n",
    "    # \"Claude 3.7 Sonnet\": \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    # \"Claude 3.5 Sonnet\": \"us.anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "    # \"Claude 3.5 Haiku\": \"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "    # \"Amazon Nova Pro\": \"us.amazon.nova-pro-v1:0\",\n",
    "    # \"Amazon Nova Micro\": \"us.amazon.nova-micro-v1:0\",\n",
    "    # \"DeepSeek-R1\": \"us.deepseek.r1-v1:0\",\n",
    "    # \"Meta Llama 3.1 70B Instruct\": \"us.meta.llama3-1-70b-instruct-v1:0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "896c75a5-06f1-40be-938a-ed0f80857071",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:43:52.005944Z",
     "iopub.status.busy": "2025-06-21T13:43:52.005581Z",
     "iopub.status.idle": "2025-06-21T13:43:52.009854Z",
     "shell.execute_reply": "2025-06-21T13:43:52.009083Z",
     "shell.execute_reply.started": "2025-06-21T13:43:52.005919Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_response(response, model_name=None):\n",
    "    if model_name:\n",
    "        display(Markdown(f\"### Response from {model_name}\"))\n",
    "    display(Markdown(response))\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa06f701-7a3b-4059-98f5-c990369c6538",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:43:53.957114Z",
     "iopub.status.busy": "2025-06-21T13:43:53.956848Z",
     "iopub.status.idle": "2025-06-21T13:43:53.960983Z",
     "shell.execute_reply": "2025-06-21T13:43:53.960056Z",
     "shell.execute_reply.started": "2025-06-21T13:43:53.957094Z"
    }
   },
   "outputs": [],
   "source": [
    "text_to_summarize = \"\"\"\n",
    "AWS took all of that feedback from customers, and today we are excited to announce Amazon Bedrock, \\\n",
    "a new service that makes FMs from AI21 Labs, Anthropic, Stability AI, and Amazon accessible via an API. \\\n",
    "Bedrock is the easiest way for customers to build and scale generative AI-based applications using FMs, \\\n",
    "democratizing access for all builders. Bedrock will offer the ability to access a range of powerful FMs \\\n",
    "for text and images—including Amazons Titan FMs, which consist of two new LLMs we're also announcing \\\n",
    "today—through a scalable, reliable, and secure AWS managed service. With Bedrock's serverless experience, \\\n",
    "customers can easily find the right model for what they're trying to get done, get started quickly, privately \\\n",
    "customize FMs with their own data, and easily integrate and deploy them into their applications using the AWS \\\n",
    "tools and capabilities they are familiar with, without having to manage any infrastructure (including integrations \\\n",
    "with Amazon SageMaker ML features like Experiments to test different models and Pipelines to manage their FMs at scale).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94f40823-89b1-45f2-8400-5edc611b7b1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:44:16.554272Z",
     "iopub.status.busy": "2025-06-21T13:44:16.553440Z",
     "iopub.status.idle": "2025-06-21T13:44:16.558166Z",
     "shell.execute_reply": "2025-06-21T13:44:16.557265Z",
     "shell.execute_reply.started": "2025-06-21T13:44:16.554241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create prompt for summarization\n",
    "prompt1 = f\"\"\"Please provide a summary of the following text. Do not add any information that is not mentioned in the text below.\n",
    "<text>\n",
    "{text_to_summarize}\n",
    "</text>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "780ef4d3-f0c4-4304-aa7f-53635243dd08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:44:18.128841Z",
     "iopub.status.busy": "2025-06-21T13:44:18.128074Z",
     "iopub.status.idle": "2025-06-21T13:44:18.131974Z",
     "shell.execute_reply": "2025-06-21T13:44:18.131084Z",
     "shell.execute_reply.started": "2025-06-21T13:44:18.128812Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt2 = \"Explain me about AWS Bedrock\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5972368-f4c7-484b-baf6-c04409ae2cb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:44:19.649868Z",
     "iopub.status.busy": "2025-06-21T13:44:19.649517Z",
     "iopub.status.idle": "2025-06-21T13:44:19.653608Z",
     "shell.execute_reply": "2025-06-21T13:44:19.652544Z",
     "shell.execute_reply.started": "2025-06-21T13:44:19.649843Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_prompt = prompt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6253bc40-1933-48e7-8128-08c0a520a44c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:44:20.920017Z",
     "iopub.status.busy": "2025-06-21T13:44:20.919655Z",
     "iopub.status.idle": "2025-06-21T13:44:20.923565Z",
     "shell.execute_reply": "2025-06-21T13:44:20.922856Z",
     "shell.execute_reply.started": "2025-06-21T13:44:20.919994Z"
    }
   },
   "outputs": [],
   "source": [
    "titan_lite_body = json.dumps({\n",
    "    \"inputText\": selected_prompt,\n",
    "    \"textGenerationConfig\": {\n",
    "        \"maxTokenCount\": 1000,\n",
    "        \"temperature\": 1,\n",
    "        \"topP\": 0.9\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a120c53-5290-4423-b1d8-4e5dfbb07c66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:44:22.350091Z",
     "iopub.status.busy": "2025-06-21T13:44:22.349454Z",
     "iopub.status.idle": "2025-06-21T13:44:22.353297Z",
     "shell.execute_reply": "2025-06-21T13:44:22.352624Z",
     "shell.execute_reply.started": "2025-06-21T13:44:22.350064Z"
    }
   },
   "outputs": [],
   "source": [
    "titan_express_body = json.dumps({\n",
    "    \"inputText\": selected_prompt,\n",
    "    \"textGenerationConfig\": {\n",
    "        \"maxTokenCount\": 1000,\n",
    "        \"temperature\": 1,\n",
    "        \"topP\": 0.9\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "352f85a0-3515-4009-bc34-38a650f7457a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:44:23.701481Z",
     "iopub.status.busy": "2025-06-21T13:44:23.701102Z",
     "iopub.status.idle": "2025-06-21T13:44:26.910178Z",
     "shell.execute_reply": "2025-06-21T13:44:26.909622Z",
     "shell.execute_reply.started": "2025-06-21T13:44:23.701459Z"
    }
   },
   "outputs": [],
   "source": [
    "# Usage of 'invoke_model' api\n",
    "\n",
    "\n",
    "titan_lite_response = bedrock.invoke_model(\n",
    "        modelId=MODELS[\"Titan Text G1 - Lite\"],\n",
    "        body=titan_lite_body,\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "titan_lite_response_body = json.loads(titan_lite_response.get('body').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "292f617c-5c07-4f07-81cf-5dca5e7554a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:44:29.989053Z",
     "iopub.status.busy": "2025-06-21T13:44:29.988783Z",
     "iopub.status.idle": "2025-06-21T13:44:34.150893Z",
     "shell.execute_reply": "2025-06-21T13:44:34.150082Z",
     "shell.execute_reply.started": "2025-06-21T13:44:29.989031Z"
    }
   },
   "outputs": [],
   "source": [
    "titan_express_response = bedrock.invoke_model(\n",
    "        modelId=MODELS[\"Titan Text G1 - Express\"],\n",
    "        body=titan_express_body,\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "titan_express_response_body = json.loads(titan_express_response.get('body').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0a45fdf-66a9-4e42-b368-8f9adc637998",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:44:51.567206Z",
     "iopub.status.busy": "2025-06-21T13:44:51.566581Z",
     "iopub.status.idle": "2025-06-21T13:44:51.570541Z",
     "shell.execute_reply": "2025-06-21T13:44:51.569962Z",
     "shell.execute_reply.started": "2025-06-21T13:44:51.567177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Amazon Bedrock is a new service from AWS that makes FMs from AI21 Labs, Anthropic, Stability AI, and Amazon accessible via an API, enabling customers to build and scale generative AI-based applications. It will offer the ability to access a range of powerful FMs for text and images through a scalable, reliable, and secure AWS managed service, with serverless experience that makes it easy to find the right model, get started quickly, customize FMs with their data, and integrate and deploy them into applications.\n"
     ]
    }
   ],
   "source": [
    "# print(response_body[\"outputText\"])\n",
    "print(titan_lite_response_body.get(\"results\")[0][\"outputText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26f0adb4-45b4-42b7-9a29-7b25728a6261",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:44:57.301261Z",
     "iopub.status.busy": "2025-06-21T13:44:57.300989Z",
     "iopub.status.idle": "2025-06-21T13:44:57.304765Z",
     "shell.execute_reply": "2025-06-21T13:44:57.304101Z",
     "shell.execute_reply.started": "2025-06-21T13:44:57.301240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Amazon Bedrock is a new service that makes generative AI-based applications using FMs accessible via an API. It offers powerful FMs for text and images, including Amazons Titan FMs, through a scalable, reliable, and secure AWS managed service. Customers can easily find the right model, get started quickly, privately customize FMs with their own data, and easily integrate and deploy them into their applications. There is no need to manage any infrastructure, including integrations with Amazon SageMaker ML features.\n"
     ]
    }
   ],
   "source": [
    "# print(response_body[\"outputText\"])\n",
    "print(titan_express_response_body.get(\"results\")[0][\"outputText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6e0fc76d-9e18-4102-a536-7807bb4ef0c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:50:44.781248Z",
     "iopub.status.busy": "2025-06-21T13:50:44.780864Z",
     "iopub.status.idle": "2025-06-21T13:50:44.784772Z",
     "shell.execute_reply": "2025-06-21T13:50:44.784203Z",
     "shell.execute_reply.started": "2025-06-21T13:50:44.781226Z"
    }
   },
   "outputs": [],
   "source": [
    "#  ----------------------------------------------------- Use of 'converse' API's ----------------------------------------------\n",
    "\n",
    "# Create a converse request with our summarization task\n",
    "converse_request = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": f\"Please provide a concise summary of the following text in 2-3 sentences. Text to summarize: {text_to_summarize}\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"inferenceConfig\": {\n",
    "        \"temperature\": 0.4,\n",
    "        \"topP\": 0.9,\n",
    "        \"maxTokens\": 500\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "325ae0e7-def1-4a07-b06f-cd7a997dc708",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T16:01:54.741971Z",
     "iopub.status.busy": "2025-06-21T16:01:54.741383Z",
     "iopub.status.idle": "2025-06-21T16:01:57.848725Z",
     "shell.execute_reply": "2025-06-21T16:01:57.848089Z",
     "shell.execute_reply.started": "2025-06-21T16:01:54.741942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Response from Titan Text G1 - Express"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Amazon Bedrock is a new service that makes AI-based applications using FMs accessible via an API, democratizing access for all builders. It offers a scalable, reliable, and secure AWS managed service with serverless experience, allowing customers to easily find the right model, get started quickly, privately customize FMs with their own data, and easily integrate and deploy them into their applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call Titan Text G1 - Express Sonnet with Converse API\n",
    "try:\n",
    "    response = bedrock.converse(\n",
    "        modelId=MODELS[\"Titan Text G1 - Express\"],\n",
    "        messages=converse_request[\"messages\"],\n",
    "        inferenceConfig=converse_request[\"inferenceConfig\"]\n",
    "    )\n",
    "    \n",
    "    # Extract the model's response\n",
    "    titan_express_converse_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    display_response(titan_express_converse_response, \"Titan Text G1 - Express\")\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41m{error.response['Error']['Code']}: {error.response['Error']['Message']}\\x1b[0m\")\n",
    "        print(\"Please ensure you have the necessary permissions for Amazon Bedrock.\")\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff046533-54ab-4dd3-8ce1-6097a405d12d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:56:04.836941Z",
     "iopub.status.busy": "2025-06-21T13:56:04.836676Z",
     "iopub.status.idle": "2025-06-21T13:56:10.078561Z",
     "shell.execute_reply": "2025-06-21T13:56:10.077698Z",
     "shell.execute_reply.started": "2025-06-21T13:56:04.836921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully called Titan Text G1 - Lite (took 2.32 seconds)\n",
      "✅ Successfully called Titan Text G1 - Express (took 2.92 seconds)\n"
     ]
    }
   ],
   "source": [
    "# call different models with the same converse request\n",
    "results = {}    \n",
    "for model_name, model_id in MODELS.items(): # looping over all models defined above\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = bedrock.converse(\n",
    "                modelId=model_id,\n",
    "                messages=converse_request[\"messages\"],\n",
    "                inferenceConfig=converse_request[\"inferenceConfig\"] if \"inferenceConfig\" in converse_request else None\n",
    "            )\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Extract the model's response using the correct structure\n",
    "            model_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "            response_time = round(end_time - start_time, 2)\n",
    "            \n",
    "            results[model_name] = {\n",
    "                \"response\": model_response,\n",
    "                \"time\": response_time\n",
    "            }\n",
    "            \n",
    "            print(f\"✅ Successfully called {model_name} (took {response_time} seconds)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error calling {model_name}: {str(e)}\")\n",
    "            results[model_name] = {\n",
    "                \"response\": f\"Error: {str(e)}\",\n",
    "                \"time\": None\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6b61c2d-daca-4115-a2d1-ac93b6e6bf85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:56:25.919996Z",
     "iopub.status.busy": "2025-06-21T13:56:25.919287Z",
     "iopub.status.idle": "2025-06-21T13:56:25.928736Z",
     "shell.execute_reply": "2025-06-21T13:56:25.927926Z",
     "shell.execute_reply.started": "2025-06-21T13:56:25.919970Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Titan Text G1 - Lite (took 2.32 seconds)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Amazon Bedrock, a new service from AWS, makes AI21 Labs, Anthropic, Stability AI, and Amazon's FMs available through an API, democratizing access for all builders. It offers a scalable, reliable, and secure AWS managed service for customers to access a range of powerful FMs for text and images, including Amazons Titan FMs, through a serverless experience."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Titan Text G1 - Express (took 2.92 seconds)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Amazon Bedrock is a new service that makes AI-based applications accessible via an API, democratizing access for all builders. It offers a scalable, reliable, and secure AWS managed service with serverless experience, allowing customers to easily find the right model, get started quickly, privately customize FMs with their own data, and easily integrate and deploy them into their applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display results in a formatted way\n",
    "for model_name, result in results.items():\n",
    "    if \"Error\" not in result[\"response\"]:\n",
    "        display(Markdown(f\"### {model_name} (took {result['time']} seconds)\"))\n",
    "        display(Markdown(result[\"response\"]))\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0965a4d-7160-4dab-a196-7985d732f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Regular model invocation (standard region)\n",
    "# standard_response = bedrock.converse(\n",
    "#     modelId=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",  # Standard model ID\n",
    "#     messages=converse_request[\"messages\"]\n",
    "# )\n",
    "\n",
    "# # Cross-region inference (note the \"us.\" prefix)\n",
    "# cris_response = bedrock.converse(\n",
    "#     modelId=\"us.anthropic.claude-3-5-sonnet-20240620-v1:0\",  # Cross-region model ID with regional prefix\n",
    "#     messages=converse_request[\"messages\"]\n",
    "# )\n",
    "\n",
    "# # Print responses\n",
    "# print(\"Standard response:\", standard_response[\"output\"][\"message\"][\"content\"][0][\"text\"])\n",
    "# print(\"Cross-region response:\", cris_response[\"output\"][\"message\"][\"content\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c87d834b-35f7-49c2-8510-1625ccc4e70e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T15:38:07.313987Z",
     "iopub.status.busy": "2025-06-21T15:38:07.313308Z",
     "iopub.status.idle": "2025-06-21T15:38:08.787586Z",
     "shell.execute_reply": "2025-06-21T15:38:08.786931Z",
     "shell.execute_reply.started": "2025-06-21T15:38:07.313953Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Response from Titan Text G1 - Express (Multi-turn conversation)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Amazon Bedrock is a new service that makes AI-based applications accessible via an API, democratizing access for all builders."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of a multi-turn conversation with Converse API\n",
    "multi_turn_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": f\"Please summarize this text: {text_to_summarize}\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"text\": results[\"Titan Text G1 - Express\"][\"response\"]}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": \"Can you make this summary even shorter, just 1 sentence?\"}]\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = bedrock.converse(\n",
    "        modelId=MODELS[\"Titan Text G1 - Express\"],\n",
    "        messages=multi_turn_messages,\n",
    "        inferenceConfig={\"temperature\": 0.2, \"maxTokens\": 500}\n",
    "    )\n",
    "    \n",
    "    # Extract the model's response using the correct structure\n",
    "    follow_up_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    display_response(follow_up_response, \"Titan Text G1 - Express (Multi-turn conversation)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bab8122e-1f79-440b-9a68-04e387ceca2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T15:37:03.188153Z",
     "iopub.status.busy": "2025-06-21T15:37:03.187373Z",
     "iopub.status.idle": "2025-06-21T15:37:03.194097Z",
     "shell.execute_reply": "2025-06-21T15:37:03.193441Z",
     "shell.execute_reply.started": "2025-06-21T15:37:03.188127Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example of streaming with Converse API\n",
    "def stream_converse(model_id, messages, inference_config=None):\n",
    "    if inference_config is None:\n",
    "        inference_config = {}\n",
    "    \n",
    "    print(\"Streaming response (chunks will appear as they are received):\\n\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    full_response = \"\"\n",
    "    \n",
    "    try:\n",
    "        response = bedrock.converse_stream(\n",
    "            modelId=model_id,\n",
    "            messages=messages,\n",
    "            inferenceConfig=inference_config\n",
    "        )\n",
    "        response_stream = response.get('stream')\n",
    "        if response_stream:\n",
    "            for event in response_stream:\n",
    "\n",
    "                if 'messageStart' in event:\n",
    "                    print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "\n",
    "                if 'contentBlockDelta' in event:\n",
    "                    print(event['contentBlockDelta']['delta']['text'], end=\"\")\n",
    "\n",
    "                if 'messageStop' in event:\n",
    "                    print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
    "\n",
    "                if 'metadata' in event:\n",
    "                    metadata = event['metadata']\n",
    "                    if 'usage' in metadata:\n",
    "                        print(\"\\nToken usage\")\n",
    "                        print(f\"Input tokens: {metadata['usage']['inputTokens']}\")\n",
    "                        print(\n",
    "                            f\":Output tokens: {metadata['usage']['outputTokens']}\")\n",
    "                        print(f\":Total tokens: {metadata['usage']['totalTokens']}\")\n",
    "                    if 'metrics' in event['metadata']:\n",
    "                        print(\n",
    "                            f\"Latency: {metadata['metrics']['latencyMs']} milliseconds\")\n",
    "\n",
    "                \n",
    "            print(\"\\n\" + \"-\" * 80)\n",
    "        return full_response\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in streaming: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4dde0b58-2f16-4c38-a963-87d23ae24274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T15:37:08.178505Z",
     "iopub.status.busy": "2025-06-21T15:37:08.178080Z",
     "iopub.status.idle": "2025-06-21T15:37:08.182652Z",
     "shell.execute_reply": "2025-06-21T15:37:08.181759Z",
     "shell.execute_reply.started": "2025-06-21T15:37:08.178481Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's try streaming a longer summary\n",
    "streaming_request = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": f\"\"\"Please provide a detailed summary of the following text, explaining its key points and implications:\n",
    "                \n",
    "                {text_to_summarize}\n",
    "                \n",
    "                Make your summary comprehensive but clear.\n",
    "                \"\"\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1dee7040-0155-4b2f-bc32-dc53c7cca6d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T15:38:25.489236Z",
     "iopub.status.busy": "2025-06-21T15:38:25.488867Z",
     "iopub.status.idle": "2025-06-21T15:38:29.903567Z",
     "shell.execute_reply": "2025-06-21T15:38:29.902845Z",
     "shell.execute_reply.started": "2025-06-21T15:38:25.489213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response (chunks will appear as they are received):\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Role: assistant\n",
      "\n",
      "Amazon Bedrock is a new service that makes AI-based applications using FMs accessible via an API, democratizing access for all builders. It offers a range of powerful FMs for text and images, including Amazons Titan FMs, through a scalable, reliable, and secure AWS managed service. With serverless experience, customers can easily find the right model for what they're trying to get done, get started quickly, privately customize FMs with their own data, and easily integrate and deploy them into their applications using AWS tools and capabilities.\n",
      "Stop reason: end_turn\n",
      "\n",
      "Token usage\n",
      "Input tokens: 250\n",
      ":Output tokens: 111\n",
      ":Total tokens: 361\n",
      "Latency: 4407 milliseconds\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Only run this when you're ready to see streaming output\n",
    "streamed_response = stream_converse(\n",
    "    MODELS[\"Titan Text G1 - Express\"], \n",
    "    streaming_request, \n",
    "    inference_config={\"temperature\": 0.4, \"maxTokens\": 1000}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7674232d-798b-43ca-a8cb-2e70a4b7e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     \"\"\"Mock function that would normally call a weather API\"\"\"\n",
    "#     print(f\"Looking up weather for {location}...\")\n",
    "    \n",
    "#     # In a real application, this would call a weather API\n",
    "#     weather_data = {\n",
    "#         \"New York\": {\"condition\": \"Partly Cloudy\", \"temperature\": 72, \"humidity\": 65},\n",
    "#         \"San Francisco\": {\"condition\": \"Foggy\", \"temperature\": 58, \"humidity\": 80},\n",
    "#         \"Miami\": {\"condition\": \"Sunny\", \"temperature\": 85, \"humidity\": 75},\n",
    "#         \"Seattle\": {\"condition\": \"Rainy\", \"temperature\": 52, \"humidity\": 90}\n",
    "#     }\n",
    "    \n",
    "#     return weather_data.get(location, {\"condition\": \"Unknown\", \"temperature\": 0, \"humidity\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ec979ecc-07de-473d-a443-19badf2dd870",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T17:05:48.164784Z",
     "iopub.status.busy": "2025-06-21T17:05:48.164428Z",
     "iopub.status.idle": "2025-06-21T17:05:48.168573Z",
     "shell.execute_reply": "2025-06-21T17:05:48.167930Z",
     "shell.execute_reply.started": "2025-06-21T17:05:48.164762Z"
    }
   },
   "outputs": [],
   "source": [
    "weather_tool = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"description\": \"Get current weather for a specific location\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The city name to get weather for\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"location\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"toolChoice\": {\n",
    "        \"auto\": {}  # Let the model decide when to use the tool\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "784eac5c-7c11-486a-b808-e0cfa7e83577",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T16:00:43.212925Z",
     "iopub.status.busy": "2025-06-21T16:00:43.212563Z",
     "iopub.status.idle": "2025-06-21T16:00:43.216241Z",
     "shell.execute_reply": "2025-06-21T16:00:43.215574Z",
     "shell.execute_reply.started": "2025-06-21T16:00:43.212901Z"
    }
   },
   "outputs": [],
   "source": [
    "function_request = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": \"What's the weather like in San Francisco right now? And what should I wear?\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"inferenceConfig\": {\n",
    "        \"temperature\": 0.0,  # Use 0 temperature for deterministic function calling\n",
    "        \"maxTokens\": 500\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ce0831d-48c5-4b1f-a4a2-b8736b35ff84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T17:06:31.594063Z",
     "iopub.status.busy": "2025-06-21T17:06:31.593671Z",
     "iopub.status.idle": "2025-06-21T17:06:31.611001Z",
     "shell.execute_reply": "2025-06-21T17:06:31.610105Z",
     "shell.execute_reply.started": "2025-06-21T17:06:31.594038Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Claude 3.7 Sonnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m response \u001b[38;5;241m=\u001b[39m bedrock\u001b[38;5;241m.\u001b[39mconverse(\n\u001b[0;32m----> 2\u001b[0m     modelId\u001b[38;5;241m=\u001b[39m\u001b[43mMODELS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClaude 3.7 Sonnet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m      3\u001b[0m     messages\u001b[38;5;241m=\u001b[39mfunction_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      4\u001b[0m     inferenceConfig\u001b[38;5;241m=\u001b[39mfunction_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferenceConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m     toolConfig\u001b[38;5;241m=\u001b[39mweather_tool\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(response, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Claude 3.7 Sonnet'"
     ]
    }
   ],
   "source": [
    "# need to have access for this 'Claude 3.7 Sonnet' model\n",
    "\n",
    "\n",
    "response = bedrock.converse(\n",
    "    modelId=MODELS[\"Claude 3.7 Sonnet\"],\n",
    "    messages=function_request[\"messages\"],\n",
    "    inferenceConfig=function_request[\"inferenceConfig\"],\n",
    "    toolConfig=weather_tool\n",
    ")\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9977cb7c-58bb-4bb7-94a8-c96b56670906",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T17:07:22.002761Z",
     "iopub.status.busy": "2025-06-21T17:07:22.002398Z",
     "iopub.status.idle": "2025-06-21T17:07:22.012532Z",
     "shell.execute_reply": "2025-06-21T17:07:22.011716Z",
     "shell.execute_reply.started": "2025-06-21T17:07:22.002734Z"
    }
   },
   "outputs": [],
   "source": [
    "def handle_function_calling(model_id, request, tool_config):\n",
    "    try:\n",
    "        # Step 1: Send initial request\n",
    "        response = bedrock.converse(\n",
    "            modelId=model_id,\n",
    "            messages=request[\"messages\"],\n",
    "            inferenceConfig=request[\"inferenceConfig\"],\n",
    "            toolConfig=tool_config\n",
    "        )\n",
    "        \n",
    "        # Check if the model wants to use a tool (check the correct response structure)\n",
    "        content_blocks = response[\"output\"][\"message\"][\"content\"]\n",
    "        has_tool_use = any(\"toolUse\" in block for block in content_blocks)\n",
    "        \n",
    "        if has_tool_use:\n",
    "            # Find the toolUse block\n",
    "            tool_use_block = next(block for block in content_blocks if \"toolUse\" in block)\n",
    "            tool_use = tool_use_block[\"toolUse\"]\n",
    "            tool_name = tool_use[\"name\"]\n",
    "            tool_input = tool_use[\"input\"]\n",
    "            tool_use_id = tool_use[\"toolUseId\"]\n",
    "            \n",
    "            # Step 2: Execute the tool\n",
    "            if tool_name == \"get_weather\":\n",
    "                tool_result = get_weather(tool_input[\"location\"])\n",
    "            else:\n",
    "                tool_result = {\"error\": f\"Unknown tool: {tool_name}\"}\n",
    "            \n",
    "            # Step 3: Send the tool result back to the model\n",
    "            updated_messages = request[\"messages\"] + [\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"toolUse\": {\n",
    "                                \"toolUseId\": tool_use_id,\n",
    "                                \"name\": tool_name,\n",
    "                                \"input\": tool_input\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"toolResult\": {\n",
    "                                \"toolUseId\": tool_use_id,\n",
    "                                \"content\": [\n",
    "                                    {\n",
    "                                        \"json\": tool_result\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"status\": \"success\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Step 4: Get final response\n",
    "            final_response = bedrock.converse(\n",
    "                modelId=model_id,\n",
    "                messages=updated_messages,\n",
    "                inferenceConfig=request[\"inferenceConfig\"],\n",
    "                toolConfig=tool_config  \n",
    "            )\n",
    "            \n",
    "            # Extract text from the correct response structure\n",
    "            final_text = \"\"\n",
    "            for block in final_response[\"output\"][\"message\"][\"content\"]:\n",
    "                if \"text\" in block:\n",
    "                    final_text = block[\"text\"]\n",
    "                    break\n",
    "            \n",
    "            return {\n",
    "                \"tool_call\": {\"name\": tool_name, \"input\": tool_input},\n",
    "                \"tool_result\": tool_result,\n",
    "                \"final_response\": final_text\n",
    "            }\n",
    "        else:\n",
    "            # Model didn't use a tool, just return the text response\n",
    "            text_response = \"\"\n",
    "            for block in content_blocks:\n",
    "                if \"text\" in block:\n",
    "                    text_response = block[\"text\"]\n",
    "                    break\n",
    "                    \n",
    "            return {\n",
    "                \"final_response\": text_response\n",
    "            }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in function calling: {str(e)}\")\n",
    "        return {\"error\": str(e)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
